{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5Jzaqkb-acN",
        "outputId": "404c3b90-6f88-46d9-faef-942446c8da9c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x1554cbb6b390>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "from copy import deepcopy\n",
        "\n",
        "n_epochs = 5 # number of epochs for training\n",
        "batch_size_train = 1024 # batch size for training\n",
        "batch_size_test = 8192 # batch size for testing\n",
        "learning_rate = 0.001 # learning rate for Adam\n",
        "log_interval = 10 # logging interval for metrics\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # device for computation\n",
        "\n",
        "# fixing random seed\n",
        "random_seed = 1\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.manual_seed(random_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MsTQf0xB-jGr"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.f1 = F1()\n",
        "        self.f2 = F2()\n",
        "    \n",
        "    def forward(self, x, y_hat, return_mid=False):\n",
        "        z = self.f1(x)\n",
        "        y_pred, z_pred  = self.f2(z, y_hat)\n",
        "        return y_pred, z_pred\n",
        "    \n",
        "    \n",
        "class F1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(F1, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.activation = nn.LeakyReLU(negative_slope=0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = self.activation(F.max_pool2d(self.conv2(x), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = self.activation(self.fc1(x))\n",
        "        return x\n",
        "    \n",
        "    \n",
        "class F2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(F2, self).__init__()\n",
        "        self.fc2 = nn.Linear(50, 50)\n",
        "        self.fc3 = nn.Linear(50, 60)\n",
        "        self.y_hat_fc = nn.Linear(10, 50)\n",
        "        self.activation = nn.LeakyReLU(negative_slope=0.01)\n",
        "\n",
        "    def forward(self, x, y_hat):\n",
        "        x = x + self.activation(self.y_hat_fc(y_hat))\n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x[:, :10].softmax(-1), x[:, 10:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def js_div(p, q):\n",
        "    m = 0.5 * (p + q)\n",
        "    return 0.5 * (F.kl_div(torch.log(p), m, reduction='batchmean') + \n",
        "                  F.kl_div(torch.log(q), m, reduction='batchmean'))\n",
        "\n",
        "def train(f, train_loader, optimizer, n_epochs, n_classes=10):\n",
        "  f.train()\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "      # randomally choose y0 to be either the true target y or a random class\n",
        "      x, y = data.to(device), target.to(device)\n",
        "      y0_d = y if torch.rand(1) > 0.25 else torch.randint(0, n_classes, (x.shape[0],), device=device)\n",
        "      y0 = F.one_hot(y0_d, num_classes=n_classes).float()\n",
        "\n",
        "      # forward pass\n",
        "      y1, z1 = f(x, y0)\n",
        "      y2, z2 = f.f2(z1, y1)\n",
        "\n",
        "      # losses\n",
        "      loss_supervised_1 = F.nll_loss(y1.log(), y)\n",
        "      loss_supervised_2 = F.nll_loss(y2.log(), y)\n",
        "      loss_unsupervised_y = js_div(y1, y2)*10\n",
        "      loss_unsupervised_z = (z1 - z2).pow(2).mean()*10\n",
        "      # loss = loss_supervised + #loss_unsupervised_y # + loss_unsupervised_z\n",
        "      loss = loss_supervised_1 + loss_supervised_2\n",
        "\n",
        "      # opt\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # log\n",
        "      if batch_idx % log_interval == 0:\n",
        "        # print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, loss_supervised: {:.6f}, loss_unsupervised_y: {:.6f}, loss_unsupervised_z: {:.6f}'.format(\n",
        "        #   epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "        #   100. * batch_idx / len(train_loader), loss.item(), loss_supervised.item(), loss_unsupervised_y.item(), loss_unsupervised_z.item()))\n",
        "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tloss_supervised_1: {:.6f}, loss_supervised_2: {:.6f}'.format(\n",
        "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "          100. * batch_idx / len(train_loader), loss_supervised_1.item(), loss_supervised_2.item()))\n",
        "        torch.save(f.state_dict(), './model.pth')\n",
        "        torch.save(optimizer.state_dict(), './optimizer.pth')\n",
        "      \n",
        "\n",
        "def test(f, test_loader, n_classes=10, with_true_y=False):\n",
        "  f.eval()\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      x, y = data.to(device), target.to(device)\n",
        "      if with_true_y:\n",
        "        y0 = F.one_hot(y, num_classes=n_classes).float()\n",
        "      else:\n",
        "        y0 = torch.ones((len(y), n_classes), device=x.device).to(device).softmax(-1)\n",
        "      y1, z1 = f(x, y0)\n",
        "      test_loss += F.nll_loss(y1.log(), y, size_average=False).item()\n",
        "      pred = y1.log().data.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(y.data.view_as(pred)).sum()\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  print(\"True y given\") if with_true_y else print(\"Max entropy given\")\n",
        "  print('\\nTest: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss, correct, len(test_loader.dataset),\n",
        "    100. * correct / len(test_loader.dataset)))\n",
        "  return 100 * correct / len(test_loader.dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-JISiC7yvlo"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lCugrW3f-xCw"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# def train(network, train_loader, optimizer, n_epochs, n_classes=10):\n",
        "#   network.train()\n",
        "#   train_losses, train_counter = [], []\n",
        "#   for epoch in range(1, n_epochs + 1):\n",
        "#     for batch_idx, (data, target) in enumerate(train_loader):\n",
        "#       # randomally choose y_hat_0 to be either the target y or max entropy\n",
        "#       x, y = data.to(device), target.to(device)\n",
        "#       y_hat_opt_a = F.one_hot(y, num_classes=n_classes).float()\n",
        "#       y_hat_opt_b = torch.ones_like(y_hat_opt_a).softmax(-1)\n",
        "#       r = (torch.rand(x.shape[0], 1, device=device) > 0).float()\n",
        "#       y_hat_0 = y_hat_opt_a * r + y_hat_opt_b * (1 - r)\n",
        "      \n",
        "#       # apply network, our prediction is called y_hat_1\n",
        "#       y_hat_1 = network(x, y_hat_0)\n",
        "#       loss = F.nll_loss(y_hat_1.log(), y)\n",
        "\n",
        "#       # opt\n",
        "#       optimizer.zero_grad()\n",
        "#       loss.backward()\n",
        "#       optimizer.step()\n",
        "\n",
        "#       # log\n",
        "#       if batch_idx % log_interval == 0:\n",
        "#         print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "#           epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "#           100. * batch_idx / len(train_loader), loss.item()))\n",
        "#         train_losses.append(loss.item())\n",
        "#         train_counter.append(\n",
        "#           (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
        "#         torch.save(network.state_dict(), './model.pth')\n",
        "#         torch.save(optimizer.state_dict(), './optimizer.pth')\n",
        "\n",
        "\n",
        "\n",
        "# def test(network, test_loader, n_classes=10, with_true_y=False):\n",
        "#   network.eval()\n",
        "#   test_loss, correct, test_losses = 0, 0, []\n",
        "#   with torch.no_grad():\n",
        "#     for data, target in test_loader:\n",
        "#       x, y = data.to(device), target.to(device)\n",
        "#       if with_true_y:\n",
        "#         y_hat_0 = F.one_hot(y, num_classes=n_classes).float()\n",
        "#       else:\n",
        "#         y_hat_0 = torch.ones((len(y), n_classes)).to(device).softmax(-1)\n",
        "#       y_hat_1 = network(x, y_hat_0)\n",
        "#       test_loss += F.nll_loss(y_hat_1.log(), y, size_average=False).item()\n",
        "#       pred = y_hat_1.log().data.max(1, keepdim=True)[1]\n",
        "#       correct += pred.eq(y.data.view_as(pred)).sum()\n",
        "#   test_loss /= len(test_loader.dataset)\n",
        "#   test_losses.append(test_loss)\n",
        "#   print(\"True y given\") if with_true_y else print(\"Max entropy given\")\n",
        "#   print('\\nTest: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "#     test_loss, correct, len(test_loader.dataset),\n",
        "#     100. * correct / len(test_loader.dataset)))\n",
        "#   return 100 * correct / len(test_loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_IS4C6B_HMp",
        "outputId": "b5252423-4826-4d3a-bc1d-c84a04b8dd6c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ashocher/anaconda3/envs/edm/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max entropy given\n",
            "\n",
            "Test: Avg. loss: 2.3083, Accuracy: 1116/10000 (11%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tloss_supervised_1: 2.331868, loss_supervised_2: 2.319867\n",
            "Train Epoch: 1 [640/60000 (1%)]\tloss_supervised_1: 2.204759, loss_supervised_2: 2.281745\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tloss_supervised_1: 1.895385, loss_supervised_2: 2.192101\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tloss_supervised_1: 1.435428, loss_supervised_2: 1.861821\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tloss_supervised_1: 0.848420, loss_supervised_2: 1.429704\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tloss_supervised_1: 0.625035, loss_supervised_2: 0.932550\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tloss_supervised_1: 0.508848, loss_supervised_2: 0.624546\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tloss_supervised_1: 0.533164, loss_supervised_2: 0.624646\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tloss_supervised_1: 0.569211, loss_supervised_2: 0.712235\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tloss_supervised_1: 0.432811, loss_supervised_2: 0.531770\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tloss_supervised_1: 0.781931, loss_supervised_2: 0.738023\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tloss_supervised_1: 0.443658, loss_supervised_2: 0.456584\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tloss_supervised_1: 0.495363, loss_supervised_2: 0.659932\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tloss_supervised_1: 0.266220, loss_supervised_2: 0.328986\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tloss_supervised_1: 0.103574, loss_supervised_2: 0.159899\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tloss_supervised_1: 0.232120, loss_supervised_2: 0.255774\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tloss_supervised_1: 0.082030, loss_supervised_2: 0.136219\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tloss_supervised_1: 0.275839, loss_supervised_2: 0.339710\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tloss_supervised_1: 0.221349, loss_supervised_2: 0.218646\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tloss_supervised_1: 0.240496, loss_supervised_2: 0.274153\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tloss_supervised_1: 0.188173, loss_supervised_2: 0.275406\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tloss_supervised_1: 0.260039, loss_supervised_2: 0.242240\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tloss_supervised_1: 0.145284, loss_supervised_2: 0.176541\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tloss_supervised_1: 0.126622, loss_supervised_2: 0.130341\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tloss_supervised_1: 0.244863, loss_supervised_2: 0.233214\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tloss_supervised_1: 0.180311, loss_supervised_2: 0.226606\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tloss_supervised_1: 0.160420, loss_supervised_2: 0.139876\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tloss_supervised_1: 0.231497, loss_supervised_2: 0.261480\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tloss_supervised_1: 0.178550, loss_supervised_2: 0.182887\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tloss_supervised_1: 0.106482, loss_supervised_2: 0.124458\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tloss_supervised_1: 0.161448, loss_supervised_2: 0.191220\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tloss_supervised_1: 0.212194, loss_supervised_2: 0.243671\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tloss_supervised_1: 0.261554, loss_supervised_2: 0.307717\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tloss_supervised_1: 0.143298, loss_supervised_2: 0.137260\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tloss_supervised_1: 0.201993, loss_supervised_2: 0.224052\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tloss_supervised_1: 0.237515, loss_supervised_2: 0.230129\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tloss_supervised_1: 0.108506, loss_supervised_2: 0.170684\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tloss_supervised_1: 0.259671, loss_supervised_2: 0.295947\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tloss_supervised_1: 0.060932, loss_supervised_2: 0.063421\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tloss_supervised_1: 0.107884, loss_supervised_2: 0.137373\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tloss_supervised_1: 0.274503, loss_supervised_2: 0.324605\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tloss_supervised_1: 0.326392, loss_supervised_2: 0.386916\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tloss_supervised_1: 0.037040, loss_supervised_2: 0.064304\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tloss_supervised_1: 0.083447, loss_supervised_2: 0.077144\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tloss_supervised_1: 0.066817, loss_supervised_2: 0.066072\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tloss_supervised_1: 0.067155, loss_supervised_2: 0.068878\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tloss_supervised_1: 0.152468, loss_supervised_2: 0.162138\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tloss_supervised_1: 0.211339, loss_supervised_2: 0.214918\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tloss_supervised_1: 0.088471, loss_supervised_2: 0.082384\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tloss_supervised_1: 0.094366, loss_supervised_2: 0.104391\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tloss_supervised_1: 0.036000, loss_supervised_2: 0.024545\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tloss_supervised_1: 0.080769, loss_supervised_2: 0.067800\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tloss_supervised_1: 0.016945, loss_supervised_2: 0.013083\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tloss_supervised_1: 0.040820, loss_supervised_2: 0.032370\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tloss_supervised_1: 0.129130, loss_supervised_2: 0.149085\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tloss_supervised_1: 0.061565, loss_supervised_2: 0.045846\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tloss_supervised_1: 0.050055, loss_supervised_2: 0.037751\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tloss_supervised_1: 0.107703, loss_supervised_2: 0.152132\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tloss_supervised_1: 0.084364, loss_supervised_2: 0.115416\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tloss_supervised_1: 0.065521, loss_supervised_2: 0.044616\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tloss_supervised_1: 0.066158, loss_supervised_2: 0.063737\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tloss_supervised_1: 0.213370, loss_supervised_2: 0.219798\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tloss_supervised_1: 0.042627, loss_supervised_2: 0.047289\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tloss_supervised_1: 0.065142, loss_supervised_2: 0.057505\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tloss_supervised_1: 0.071537, loss_supervised_2: 0.103599\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tloss_supervised_1: 0.092053, loss_supervised_2: 0.077134\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tloss_supervised_1: 0.094417, loss_supervised_2: 0.110435\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tloss_supervised_1: 0.050425, loss_supervised_2: 0.035260\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tloss_supervised_1: 0.025500, loss_supervised_2: 0.023000\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tloss_supervised_1: 0.034122, loss_supervised_2: 0.021268\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tloss_supervised_1: 0.067678, loss_supervised_2: 0.047166\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tloss_supervised_1: 0.080430, loss_supervised_2: 0.073073\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tloss_supervised_1: 0.170065, loss_supervised_2: 0.157504\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tloss_supervised_1: 0.110476, loss_supervised_2: 0.103571\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tloss_supervised_1: 0.057343, loss_supervised_2: 0.039752\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tloss_supervised_1: 0.016622, loss_supervised_2: 0.020262\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tloss_supervised_1: 0.033142, loss_supervised_2: 0.020660\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tloss_supervised_1: 0.145108, loss_supervised_2: 0.159463\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tloss_supervised_1: 0.101474, loss_supervised_2: 0.084260\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tloss_supervised_1: 0.040540, loss_supervised_2: 0.044981\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tloss_supervised_1: 0.124412, loss_supervised_2: 0.126468\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tloss_supervised_1: 0.020377, loss_supervised_2: 0.012604\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tloss_supervised_1: 0.045144, loss_supervised_2: 0.027772\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tloss_supervised_1: 0.031902, loss_supervised_2: 0.054647\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tloss_supervised_1: 0.089671, loss_supervised_2: 0.091417\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tloss_supervised_1: 0.031782, loss_supervised_2: 0.017646\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tloss_supervised_1: 0.122319, loss_supervised_2: 0.098958\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tloss_supervised_1: 0.176413, loss_supervised_2: 0.198728\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tloss_supervised_1: 0.096812, loss_supervised_2: 0.103022\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tloss_supervised_1: 0.050435, loss_supervised_2: 0.046082\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tloss_supervised_1: 0.007424, loss_supervised_2: 0.003506\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tloss_supervised_1: 0.010166, loss_supervised_2: 0.004432\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tloss_supervised_1: 0.020381, loss_supervised_2: 0.020813\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tloss_supervised_1: 0.079329, loss_supervised_2: 0.106461\n",
            "Train Epoch: 2 [0/60000 (0%)]\tloss_supervised_1: 0.049483, loss_supervised_2: 0.032120\n",
            "Train Epoch: 2 [640/60000 (1%)]\tloss_supervised_1: 0.013291, loss_supervised_2: 0.006531\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tloss_supervised_1: 0.115863, loss_supervised_2: 0.103981\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tloss_supervised_1: 0.016155, loss_supervised_2: 0.009331\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tloss_supervised_1: 0.066398, loss_supervised_2: 0.049585\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tloss_supervised_1: 0.040548, loss_supervised_2: 0.032499\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tloss_supervised_1: 0.101713, loss_supervised_2: 0.096218\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tloss_supervised_1: 0.006699, loss_supervised_2: 0.004826\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tloss_supervised_1: 0.032700, loss_supervised_2: 0.027963\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tloss_supervised_1: 0.047282, loss_supervised_2: 0.049471\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tloss_supervised_1: 0.064736, loss_supervised_2: 0.073978\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tloss_supervised_1: 0.070437, loss_supervised_2: 0.091269\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tloss_supervised_1: 0.096868, loss_supervised_2: 0.102925\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tloss_supervised_1: 0.013360, loss_supervised_2: 0.010405\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tloss_supervised_1: 0.006701, loss_supervised_2: 0.005149\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tloss_supervised_1: 0.085836, loss_supervised_2: 0.114197\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tloss_supervised_1: 0.066911, loss_supervised_2: 0.058629\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tloss_supervised_1: 0.035652, loss_supervised_2: 0.050399\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tloss_supervised_1: 0.065587, loss_supervised_2: 0.062169\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tloss_supervised_1: 0.088422, loss_supervised_2: 0.080174\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tloss_supervised_1: 0.104358, loss_supervised_2: 0.134457\n",
            "Train Epoch: 2 [13440/60000 (22%)]\tloss_supervised_1: 0.037017, loss_supervised_2: 0.026030\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tloss_supervised_1: 0.046194, loss_supervised_2: 0.049481\n",
            "Train Epoch: 2 [14720/60000 (25%)]\tloss_supervised_1: 0.016808, loss_supervised_2: 0.013354\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tloss_supervised_1: 0.004485, loss_supervised_2: 0.001484\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tloss_supervised_1: 0.178032, loss_supervised_2: 0.196549\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tloss_supervised_1: 0.173031, loss_supervised_2: 0.211829\n",
            "Train Epoch: 2 [17280/60000 (29%)]\tloss_supervised_1: 0.031409, loss_supervised_2: 0.040020\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tloss_supervised_1: 0.035164, loss_supervised_2: 0.033375\n",
            "Train Epoch: 2 [18560/60000 (31%)]\tloss_supervised_1: 0.012323, loss_supervised_2: 0.006301\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tloss_supervised_1: 0.015152, loss_supervised_2: 0.013164\n",
            "Train Epoch: 2 [19840/60000 (33%)]\tloss_supervised_1: 0.024093, loss_supervised_2: 0.019618\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tloss_supervised_1: 0.069989, loss_supervised_2: 0.059915\n",
            "Train Epoch: 2 [21120/60000 (35%)]\tloss_supervised_1: 0.151296, loss_supervised_2: 0.217636\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tloss_supervised_1: 0.107569, loss_supervised_2: 0.149911\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tloss_supervised_1: 0.006737, loss_supervised_2: 0.002969\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tloss_supervised_1: 0.042317, loss_supervised_2: 0.044180\n",
            "Train Epoch: 2 [23680/60000 (39%)]\tloss_supervised_1: 0.023534, loss_supervised_2: 0.041194\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tloss_supervised_1: 0.026677, loss_supervised_2: 0.032030\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tloss_supervised_1: 0.051624, loss_supervised_2: 0.088275\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tloss_supervised_1: 0.023223, loss_supervised_2: 0.021750\n",
            "Train Epoch: 2 [26240/60000 (44%)]\tloss_supervised_1: 0.019713, loss_supervised_2: 0.012231\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tloss_supervised_1: 0.054426, loss_supervised_2: 0.082174\n",
            "Train Epoch: 2 [27520/60000 (46%)]\tloss_supervised_1: 0.093821, loss_supervised_2: 0.074872\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tloss_supervised_1: 0.008198, loss_supervised_2: 0.002834\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tloss_supervised_1: 0.011199, loss_supervised_2: 0.004814\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tloss_supervised_1: 0.044821, loss_supervised_2: 0.034202\n",
            "Train Epoch: 2 [30080/60000 (50%)]\tloss_supervised_1: 0.006546, loss_supervised_2: 0.002411\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tloss_supervised_1: 0.090176, loss_supervised_2: 0.103898\n",
            "Train Epoch: 2 [31360/60000 (52%)]\tloss_supervised_1: 0.016477, loss_supervised_2: 0.011005\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tloss_supervised_1: 0.046379, loss_supervised_2: 0.063122\n",
            "Train Epoch: 2 [32640/60000 (54%)]\tloss_supervised_1: 0.061369, loss_supervised_2: 0.083653\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tloss_supervised_1: 0.087786, loss_supervised_2: 0.109589\n",
            "Train Epoch: 2 [33920/60000 (57%)]\tloss_supervised_1: 0.146113, loss_supervised_2: 0.192607\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tloss_supervised_1: 0.120450, loss_supervised_2: 0.114553\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tloss_supervised_1: 0.010381, loss_supervised_2: 0.004660\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tloss_supervised_1: 0.004921, loss_supervised_2: 0.002274\n",
            "Train Epoch: 2 [36480/60000 (61%)]\tloss_supervised_1: 0.004071, loss_supervised_2: 0.001160\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tloss_supervised_1: 0.051000, loss_supervised_2: 0.034444\n",
            "Train Epoch: 2 [37760/60000 (63%)]\tloss_supervised_1: 0.096928, loss_supervised_2: 0.093793\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tloss_supervised_1: 0.144370, loss_supervised_2: 0.155922\n",
            "Train Epoch: 2 [39040/60000 (65%)]\tloss_supervised_1: 0.059007, loss_supervised_2: 0.082487\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tloss_supervised_1: 0.016409, loss_supervised_2: 0.008444\n",
            "Train Epoch: 2 [40320/60000 (67%)]\tloss_supervised_1: 0.129024, loss_supervised_2: 0.132564\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tloss_supervised_1: 0.008508, loss_supervised_2: 0.003525\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tloss_supervised_1: 0.008095, loss_supervised_2: 0.005744\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tloss_supervised_1: 0.069914, loss_supervised_2: 0.071009\n",
            "Train Epoch: 2 [42880/60000 (71%)]\tloss_supervised_1: 0.013670, loss_supervised_2: 0.005809\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tloss_supervised_1: 0.092925, loss_supervised_2: 0.126584\n",
            "Train Epoch: 2 [44160/60000 (74%)]\tloss_supervised_1: 0.037781, loss_supervised_2: 0.088271\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tloss_supervised_1: 0.087626, loss_supervised_2: 0.115513\n",
            "Train Epoch: 2 [45440/60000 (76%)]\tloss_supervised_1: 0.010304, loss_supervised_2: 0.005178\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tloss_supervised_1: 0.123980, loss_supervised_2: 0.141540\n",
            "Train Epoch: 2 [46720/60000 (78%)]\tloss_supervised_1: 0.011866, loss_supervised_2: 0.009597\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tloss_supervised_1: 0.022772, loss_supervised_2: 0.010938\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tloss_supervised_1: 0.018660, loss_supervised_2: 0.030262\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tloss_supervised_1: 0.018134, loss_supervised_2: 0.008562\n",
            "Train Epoch: 2 [49280/60000 (82%)]\tloss_supervised_1: 0.019190, loss_supervised_2: 0.015534\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tloss_supervised_1: 0.025453, loss_supervised_2: 0.026027\n",
            "Train Epoch: 2 [50560/60000 (84%)]\tloss_supervised_1: 0.121241, loss_supervised_2: 0.144696\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tloss_supervised_1: 0.043101, loss_supervised_2: 0.035519\n",
            "Train Epoch: 2 [51840/60000 (86%)]\tloss_supervised_1: 0.004693, loss_supervised_2: 0.002546\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tloss_supervised_1: 0.003807, loss_supervised_2: 0.002398\n",
            "Train Epoch: 2 [53120/60000 (88%)]\tloss_supervised_1: 0.090271, loss_supervised_2: 0.097229\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tloss_supervised_1: 0.016959, loss_supervised_2: 0.005318\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tloss_supervised_1: 0.097121, loss_supervised_2: 0.116447\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tloss_supervised_1: 0.006264, loss_supervised_2: 0.002487\n",
            "Train Epoch: 2 [55680/60000 (93%)]\tloss_supervised_1: 0.013829, loss_supervised_2: 0.005561\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tloss_supervised_1: 0.025190, loss_supervised_2: 0.035696\n",
            "Train Epoch: 2 [56960/60000 (95%)]\tloss_supervised_1: 0.037225, loss_supervised_2: 0.031290\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tloss_supervised_1: 0.031662, loss_supervised_2: 0.057202\n",
            "Train Epoch: 2 [58240/60000 (97%)]\tloss_supervised_1: 0.165588, loss_supervised_2: 0.172242\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tloss_supervised_1: 0.022119, loss_supervised_2: 0.028201\n",
            "Train Epoch: 2 [59520/60000 (99%)]\tloss_supervised_1: 0.041878, loss_supervised_2: 0.030425\n",
            "Train Epoch: 3 [0/60000 (0%)]\tloss_supervised_1: 0.068160, loss_supervised_2: 0.050947\n",
            "Train Epoch: 3 [640/60000 (1%)]\tloss_supervised_1: 0.101567, loss_supervised_2: 0.121906\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tloss_supervised_1: 0.004288, loss_supervised_2: 0.003603\n",
            "Train Epoch: 3 [1920/60000 (3%)]\tloss_supervised_1: 0.010832, loss_supervised_2: 0.009529\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tloss_supervised_1: 0.012652, loss_supervised_2: 0.007387\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tloss_supervised_1: 0.014407, loss_supervised_2: 0.010852\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tloss_supervised_1: 0.034502, loss_supervised_2: 0.035950\n",
            "Train Epoch: 3 [4480/60000 (7%)]\tloss_supervised_1: 0.003684, loss_supervised_2: 0.001760\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tloss_supervised_1: 0.013860, loss_supervised_2: 0.017722\n",
            "Train Epoch: 3 [5760/60000 (10%)]\tloss_supervised_1: 0.048139, loss_supervised_2: 0.031499\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tloss_supervised_1: 0.039109, loss_supervised_2: 0.042776\n",
            "Train Epoch: 3 [7040/60000 (12%)]\tloss_supervised_1: 0.024885, loss_supervised_2: 0.027600\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tloss_supervised_1: 0.048960, loss_supervised_2: 0.050370\n",
            "Train Epoch: 3 [8320/60000 (14%)]\tloss_supervised_1: 0.060928, loss_supervised_2: 0.112259\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tloss_supervised_1: 0.005675, loss_supervised_2: 0.004762\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tloss_supervised_1: 0.012254, loss_supervised_2: 0.008776\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tloss_supervised_1: 0.055767, loss_supervised_2: 0.065412\n",
            "Train Epoch: 3 [10880/60000 (18%)]\tloss_supervised_1: 0.218581, loss_supervised_2: 0.222130\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tloss_supervised_1: 0.002259, loss_supervised_2: 0.001120\n",
            "Train Epoch: 3 [12160/60000 (20%)]\tloss_supervised_1: 0.003615, loss_supervised_2: 0.001596\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tloss_supervised_1: 0.217926, loss_supervised_2: 0.261494\n",
            "Train Epoch: 3 [13440/60000 (22%)]\tloss_supervised_1: 0.003056, loss_supervised_2: 0.002247\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tloss_supervised_1: 0.016653, loss_supervised_2: 0.014921\n",
            "Train Epoch: 3 [14720/60000 (25%)]\tloss_supervised_1: 0.034083, loss_supervised_2: 0.039489\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tloss_supervised_1: 0.007502, loss_supervised_2: 0.007646\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tloss_supervised_1: 0.152681, loss_supervised_2: 0.196066\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tloss_supervised_1: 0.015572, loss_supervised_2: 0.008338\n",
            "Train Epoch: 3 [17280/60000 (29%)]\tloss_supervised_1: 0.056344, loss_supervised_2: 0.086051\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tloss_supervised_1: 0.007604, loss_supervised_2: 0.005988\n",
            "Train Epoch: 3 [18560/60000 (31%)]\tloss_supervised_1: 0.024224, loss_supervised_2: 0.032390\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tloss_supervised_1: 0.157395, loss_supervised_2: 0.184555\n",
            "Train Epoch: 3 [19840/60000 (33%)]\tloss_supervised_1: 0.069146, loss_supervised_2: 0.075508\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tloss_supervised_1: 0.015578, loss_supervised_2: 0.014775\n",
            "Train Epoch: 3 [21120/60000 (35%)]\tloss_supervised_1: 0.037847, loss_supervised_2: 0.027845\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tloss_supervised_1: 0.026321, loss_supervised_2: 0.025118\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tloss_supervised_1: 0.068798, loss_supervised_2: 0.088357\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tloss_supervised_1: 0.039372, loss_supervised_2: 0.035568\n",
            "Train Epoch: 3 [23680/60000 (39%)]\tloss_supervised_1: 0.005529, loss_supervised_2: 0.003434\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tloss_supervised_1: 0.007078, loss_supervised_2: 0.003862\n",
            "Train Epoch: 3 [24960/60000 (42%)]\tloss_supervised_1: 0.219792, loss_supervised_2: 0.284123\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tloss_supervised_1: 0.003347, loss_supervised_2: 0.001551\n",
            "Train Epoch: 3 [26240/60000 (44%)]\tloss_supervised_1: 0.011906, loss_supervised_2: 0.012061\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tloss_supervised_1: 0.006977, loss_supervised_2: 0.005099\n",
            "Train Epoch: 3 [27520/60000 (46%)]\tloss_supervised_1: 0.016574, loss_supervised_2: 0.008396\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tloss_supervised_1: 0.043278, loss_supervised_2: 0.065721\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tloss_supervised_1: 0.066819, loss_supervised_2: 0.056408\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tloss_supervised_1: 0.005927, loss_supervised_2: 0.005827\n",
            "Train Epoch: 3 [30080/60000 (50%)]\tloss_supervised_1: 0.018657, loss_supervised_2: 0.021891\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tloss_supervised_1: 0.002179, loss_supervised_2: 0.001310\n",
            "Train Epoch: 3 [31360/60000 (52%)]\tloss_supervised_1: 0.212370, loss_supervised_2: 0.307531\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tloss_supervised_1: 0.023746, loss_supervised_2: 0.012036\n",
            "Train Epoch: 3 [32640/60000 (54%)]\tloss_supervised_1: 0.009615, loss_supervised_2: 0.008892\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tloss_supervised_1: 0.000929, loss_supervised_2: 0.000694\n",
            "Train Epoch: 3 [33920/60000 (57%)]\tloss_supervised_1: 0.034828, loss_supervised_2: 0.032321\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tloss_supervised_1: 0.007109, loss_supervised_2: 0.010952\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tloss_supervised_1: 0.017636, loss_supervised_2: 0.020830\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tloss_supervised_1: 0.010515, loss_supervised_2: 0.005072\n",
            "Train Epoch: 3 [36480/60000 (61%)]\tloss_supervised_1: 0.076692, loss_supervised_2: 0.073621\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tloss_supervised_1: 0.095808, loss_supervised_2: 0.107739\n",
            "Train Epoch: 3 [37760/60000 (63%)]\tloss_supervised_1: 0.022008, loss_supervised_2: 0.012322\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tloss_supervised_1: 0.064564, loss_supervised_2: 0.067697\n",
            "Train Epoch: 3 [39040/60000 (65%)]\tloss_supervised_1: 0.028146, loss_supervised_2: 0.015753\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tloss_supervised_1: 0.001506, loss_supervised_2: 0.000827\n",
            "Train Epoch: 3 [40320/60000 (67%)]\tloss_supervised_1: 0.004307, loss_supervised_2: 0.001657\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tloss_supervised_1: 0.011794, loss_supervised_2: 0.002517\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tloss_supervised_1: 0.015089, loss_supervised_2: 0.012890\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tloss_supervised_1: 0.087400, loss_supervised_2: 0.080874\n",
            "Train Epoch: 3 [42880/60000 (71%)]\tloss_supervised_1: 0.007254, loss_supervised_2: 0.010084\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tloss_supervised_1: 0.033473, loss_supervised_2: 0.016687\n",
            "Train Epoch: 3 [44160/60000 (74%)]\tloss_supervised_1: 0.066036, loss_supervised_2: 0.051604\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tloss_supervised_1: 0.007296, loss_supervised_2: 0.007543\n",
            "Train Epoch: 3 [45440/60000 (76%)]\tloss_supervised_1: 0.030248, loss_supervised_2: 0.028442\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tloss_supervised_1: 0.124001, loss_supervised_2: 0.151391\n",
            "Train Epoch: 3 [46720/60000 (78%)]\tloss_supervised_1: 0.026814, loss_supervised_2: 0.045288\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tloss_supervised_1: 0.113801, loss_supervised_2: 0.139953\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tloss_supervised_1: 0.021984, loss_supervised_2: 0.008892\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tloss_supervised_1: 0.002537, loss_supervised_2: 0.001855\n",
            "Train Epoch: 3 [49280/60000 (82%)]\tloss_supervised_1: 0.020336, loss_supervised_2: 0.021156\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tloss_supervised_1: 0.015587, loss_supervised_2: 0.008417\n",
            "Train Epoch: 3 [50560/60000 (84%)]\tloss_supervised_1: 0.050275, loss_supervised_2: 0.058521\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tloss_supervised_1: 0.006707, loss_supervised_2: 0.006798\n",
            "Train Epoch: 3 [51840/60000 (86%)]\tloss_supervised_1: 0.028080, loss_supervised_2: 0.036627\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tloss_supervised_1: 0.092004, loss_supervised_2: 0.101088\n",
            "Train Epoch: 3 [53120/60000 (88%)]\tloss_supervised_1: 0.002853, loss_supervised_2: 0.003561\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tloss_supervised_1: 0.001424, loss_supervised_2: 0.001112\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tloss_supervised_1: 0.005451, loss_supervised_2: 0.002314\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tloss_supervised_1: 0.015319, loss_supervised_2: 0.006511\n",
            "Train Epoch: 3 [55680/60000 (93%)]\tloss_supervised_1: 0.077116, loss_supervised_2: 0.098552\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tloss_supervised_1: 0.000887, loss_supervised_2: 0.000659\n",
            "Train Epoch: 3 [56960/60000 (95%)]\tloss_supervised_1: 0.007320, loss_supervised_2: 0.003697\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tloss_supervised_1: 0.001569, loss_supervised_2: 0.001601\n",
            "Train Epoch: 3 [58240/60000 (97%)]\tloss_supervised_1: 0.006388, loss_supervised_2: 0.003874\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tloss_supervised_1: 0.002368, loss_supervised_2: 0.001811\n",
            "Train Epoch: 3 [59520/60000 (99%)]\tloss_supervised_1: 0.012053, loss_supervised_2: 0.008209\n",
            "Train Epoch: 4 [0/60000 (0%)]\tloss_supervised_1: 0.007251, loss_supervised_2: 0.012214\n",
            "Train Epoch: 4 [640/60000 (1%)]\tloss_supervised_1: 0.056496, loss_supervised_2: 0.092878\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tloss_supervised_1: 0.002582, loss_supervised_2: 0.001380\n",
            "Train Epoch: 4 [1920/60000 (3%)]\tloss_supervised_1: 0.045857, loss_supervised_2: 0.029571\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tloss_supervised_1: 0.015676, loss_supervised_2: 0.016323\n",
            "Train Epoch: 4 [3200/60000 (5%)]\tloss_supervised_1: 0.101522, loss_supervised_2: 0.137030\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tloss_supervised_1: 0.047338, loss_supervised_2: 0.058314\n",
            "Train Epoch: 4 [4480/60000 (7%)]\tloss_supervised_1: 0.057036, loss_supervised_2: 0.067411\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tloss_supervised_1: 0.042820, loss_supervised_2: 0.049666\n",
            "Train Epoch: 4 [5760/60000 (10%)]\tloss_supervised_1: 0.021530, loss_supervised_2: 0.025664\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tloss_supervised_1: 0.066541, loss_supervised_2: 0.051767\n",
            "Train Epoch: 4 [7040/60000 (12%)]\tloss_supervised_1: 0.014643, loss_supervised_2: 0.020673\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tloss_supervised_1: 0.069362, loss_supervised_2: 0.090228\n",
            "Train Epoch: 4 [8320/60000 (14%)]\tloss_supervised_1: 0.002211, loss_supervised_2: 0.001247\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tloss_supervised_1: 0.002503, loss_supervised_2: 0.001556\n",
            "Train Epoch: 4 [9600/60000 (16%)]\tloss_supervised_1: 0.028049, loss_supervised_2: 0.039518\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tloss_supervised_1: 0.017408, loss_supervised_2: 0.023810\n",
            "Train Epoch: 4 [10880/60000 (18%)]\tloss_supervised_1: 0.001560, loss_supervised_2: 0.000770\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tloss_supervised_1: 0.003410, loss_supervised_2: 0.002307\n",
            "Train Epoch: 4 [12160/60000 (20%)]\tloss_supervised_1: 0.002645, loss_supervised_2: 0.001510\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tloss_supervised_1: 0.007575, loss_supervised_2: 0.007192\n",
            "Train Epoch: 4 [13440/60000 (22%)]\tloss_supervised_1: 0.006351, loss_supervised_2: 0.003712\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tloss_supervised_1: 0.013988, loss_supervised_2: 0.020213\n",
            "Train Epoch: 4 [14720/60000 (25%)]\tloss_supervised_1: 0.022761, loss_supervised_2: 0.022159\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tloss_supervised_1: 0.006502, loss_supervised_2: 0.006856\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tloss_supervised_1: 0.000929, loss_supervised_2: 0.000509\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tloss_supervised_1: 0.029446, loss_supervised_2: 0.047877\n",
            "Train Epoch: 4 [17280/60000 (29%)]\tloss_supervised_1: 0.027088, loss_supervised_2: 0.014974\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tloss_supervised_1: 0.022473, loss_supervised_2: 0.016842\n",
            "Train Epoch: 4 [18560/60000 (31%)]\tloss_supervised_1: 0.000698, loss_supervised_2: 0.000519\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tloss_supervised_1: 0.002249, loss_supervised_2: 0.001895\n",
            "Train Epoch: 4 [19840/60000 (33%)]\tloss_supervised_1: 0.001253, loss_supervised_2: 0.000513\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tloss_supervised_1: 0.053693, loss_supervised_2: 0.077228\n",
            "Train Epoch: 4 [21120/60000 (35%)]\tloss_supervised_1: 0.005553, loss_supervised_2: 0.003844\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tloss_supervised_1: 0.004618, loss_supervised_2: 0.003005\n",
            "Train Epoch: 4 [22400/60000 (37%)]\tloss_supervised_1: 0.004182, loss_supervised_2: 0.002418\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tloss_supervised_1: 0.071023, loss_supervised_2: 0.053480\n",
            "Train Epoch: 4 [23680/60000 (39%)]\tloss_supervised_1: 0.070041, loss_supervised_2: 0.108931\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tloss_supervised_1: 0.011036, loss_supervised_2: 0.016664\n",
            "Train Epoch: 4 [24960/60000 (42%)]\tloss_supervised_1: 0.033237, loss_supervised_2: 0.055811\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tloss_supervised_1: 0.003402, loss_supervised_2: 0.007871\n",
            "Train Epoch: 4 [26240/60000 (44%)]\tloss_supervised_1: 0.058801, loss_supervised_2: 0.085505\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tloss_supervised_1: 0.095385, loss_supervised_2: 0.122166\n",
            "Train Epoch: 4 [27520/60000 (46%)]\tloss_supervised_1: 0.002426, loss_supervised_2: 0.001544\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tloss_supervised_1: 0.048096, loss_supervised_2: 0.097389\n",
            "Train Epoch: 4 [28800/60000 (48%)]\tloss_supervised_1: 0.030953, loss_supervised_2: 0.025857\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tloss_supervised_1: 0.005004, loss_supervised_2: 0.006671\n",
            "Train Epoch: 4 [30080/60000 (50%)]\tloss_supervised_1: 0.076305, loss_supervised_2: 0.086635\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tloss_supervised_1: 0.128051, loss_supervised_2: 0.139722\n",
            "Train Epoch: 4 [31360/60000 (52%)]\tloss_supervised_1: 0.014734, loss_supervised_2: 0.017057\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tloss_supervised_1: 0.001232, loss_supervised_2: 0.001118\n",
            "Train Epoch: 4 [32640/60000 (54%)]\tloss_supervised_1: 0.000940, loss_supervised_2: 0.000805\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tloss_supervised_1: 0.001523, loss_supervised_2: 0.000600\n",
            "Train Epoch: 4 [33920/60000 (57%)]\tloss_supervised_1: 0.001840, loss_supervised_2: 0.001769\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tloss_supervised_1: 0.002349, loss_supervised_2: 0.001136\n",
            "Train Epoch: 4 [35200/60000 (59%)]\tloss_supervised_1: 0.071652, loss_supervised_2: 0.065051\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tloss_supervised_1: 0.006576, loss_supervised_2: 0.015180\n",
            "Train Epoch: 4 [36480/60000 (61%)]\tloss_supervised_1: 0.010599, loss_supervised_2: 0.012551\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tloss_supervised_1: 0.078534, loss_supervised_2: 0.129046\n",
            "Train Epoch: 4 [37760/60000 (63%)]\tloss_supervised_1: 0.001349, loss_supervised_2: 0.001152\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tloss_supervised_1: 0.033081, loss_supervised_2: 0.054682\n",
            "Train Epoch: 4 [39040/60000 (65%)]\tloss_supervised_1: 0.006626, loss_supervised_2: 0.004057\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tloss_supervised_1: 0.001743, loss_supervised_2: 0.001669\n",
            "Train Epoch: 4 [40320/60000 (67%)]\tloss_supervised_1: 0.000610, loss_supervised_2: 0.000709\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tloss_supervised_1: 0.001258, loss_supervised_2: 0.001110\n",
            "Train Epoch: 4 [41600/60000 (69%)]\tloss_supervised_1: 0.003555, loss_supervised_2: 0.001551\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tloss_supervised_1: 0.006849, loss_supervised_2: 0.015176\n",
            "Train Epoch: 4 [42880/60000 (71%)]\tloss_supervised_1: 0.006457, loss_supervised_2: 0.002609\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tloss_supervised_1: 0.001189, loss_supervised_2: 0.000538\n",
            "Train Epoch: 4 [44160/60000 (74%)]\tloss_supervised_1: 0.003201, loss_supervised_2: 0.001165\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tloss_supervised_1: 0.056168, loss_supervised_2: 0.057481\n",
            "Train Epoch: 4 [45440/60000 (76%)]\tloss_supervised_1: 0.017870, loss_supervised_2: 0.020543\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tloss_supervised_1: 0.054312, loss_supervised_2: 0.080642\n",
            "Train Epoch: 4 [46720/60000 (78%)]\tloss_supervised_1: 0.054742, loss_supervised_2: 0.057074\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tloss_supervised_1: 0.003658, loss_supervised_2: 0.001774\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tloss_supervised_1: 0.045499, loss_supervised_2: 0.067869\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tloss_supervised_1: 0.091933, loss_supervised_2: 0.099980\n",
            "Train Epoch: 4 [49280/60000 (82%)]\tloss_supervised_1: 0.008497, loss_supervised_2: 0.009823\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tloss_supervised_1: 0.060756, loss_supervised_2: 0.079953\n",
            "Train Epoch: 4 [50560/60000 (84%)]\tloss_supervised_1: 0.007052, loss_supervised_2: 0.004317\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tloss_supervised_1: 0.032931, loss_supervised_2: 0.074967\n",
            "Train Epoch: 4 [51840/60000 (86%)]\tloss_supervised_1: 0.001438, loss_supervised_2: 0.000772\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tloss_supervised_1: 0.067452, loss_supervised_2: 0.074939\n",
            "Train Epoch: 4 [53120/60000 (88%)]\tloss_supervised_1: 0.002018, loss_supervised_2: 0.000938\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tloss_supervised_1: 0.005787, loss_supervised_2: 0.009108\n",
            "Train Epoch: 4 [54400/60000 (91%)]\tloss_supervised_1: 0.014469, loss_supervised_2: 0.021937\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tloss_supervised_1: 0.003045, loss_supervised_2: 0.000688\n",
            "Train Epoch: 4 [55680/60000 (93%)]\tloss_supervised_1: 0.025104, loss_supervised_2: 0.018711\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tloss_supervised_1: 0.033260, loss_supervised_2: 0.027504\n",
            "Train Epoch: 4 [56960/60000 (95%)]\tloss_supervised_1: 0.009037, loss_supervised_2: 0.011682\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tloss_supervised_1: 0.012991, loss_supervised_2: 0.010778\n",
            "Train Epoch: 4 [58240/60000 (97%)]\tloss_supervised_1: 0.039658, loss_supervised_2: 0.073626\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tloss_supervised_1: 0.007803, loss_supervised_2: 0.006349\n",
            "Train Epoch: 4 [59520/60000 (99%)]\tloss_supervised_1: 0.001345, loss_supervised_2: 0.001098\n",
            "Train Epoch: 5 [0/60000 (0%)]\tloss_supervised_1: 0.002482, loss_supervised_2: 0.003258\n",
            "Train Epoch: 5 [640/60000 (1%)]\tloss_supervised_1: 0.002233, loss_supervised_2: 0.002194\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tloss_supervised_1: 0.002894, loss_supervised_2: 0.001916\n",
            "Train Epoch: 5 [1920/60000 (3%)]\tloss_supervised_1: 0.062403, loss_supervised_2: 0.076490\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tloss_supervised_1: 0.001780, loss_supervised_2: 0.001907\n",
            "Train Epoch: 5 [3200/60000 (5%)]\tloss_supervised_1: 0.002105, loss_supervised_2: 0.001677\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tloss_supervised_1: 0.001035, loss_supervised_2: 0.001790\n",
            "Train Epoch: 5 [4480/60000 (7%)]\tloss_supervised_1: 0.013435, loss_supervised_2: 0.024362\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tloss_supervised_1: 0.041349, loss_supervised_2: 0.061919\n",
            "Train Epoch: 5 [5760/60000 (10%)]\tloss_supervised_1: 0.004552, loss_supervised_2: 0.003441\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tloss_supervised_1: 0.001682, loss_supervised_2: 0.001109\n",
            "Train Epoch: 5 [7040/60000 (12%)]\tloss_supervised_1: 0.000986, loss_supervised_2: 0.000727\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tloss_supervised_1: 0.003552, loss_supervised_2: 0.002704\n",
            "Train Epoch: 5 [8320/60000 (14%)]\tloss_supervised_1: 0.062444, loss_supervised_2: 0.107599\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tloss_supervised_1: 0.001585, loss_supervised_2: 0.000776\n",
            "Train Epoch: 5 [9600/60000 (16%)]\tloss_supervised_1: 0.008534, loss_supervised_2: 0.021357\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tloss_supervised_1: 0.000376, loss_supervised_2: 0.000585\n",
            "Train Epoch: 5 [10880/60000 (18%)]\tloss_supervised_1: 0.018370, loss_supervised_2: 0.023650\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tloss_supervised_1: 0.034747, loss_supervised_2: 0.049973\n",
            "Train Epoch: 5 [12160/60000 (20%)]\tloss_supervised_1: 0.000924, loss_supervised_2: 0.000676\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tloss_supervised_1: 0.034353, loss_supervised_2: 0.057182\n",
            "Train Epoch: 5 [13440/60000 (22%)]\tloss_supervised_1: 0.000819, loss_supervised_2: 0.000595\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tloss_supervised_1: 0.000385, loss_supervised_2: 0.000324\n",
            "Train Epoch: 5 [14720/60000 (25%)]\tloss_supervised_1: 0.006808, loss_supervised_2: 0.004430\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tloss_supervised_1: 0.093519, loss_supervised_2: 0.133728\n",
            "Train Epoch: 5 [16000/60000 (27%)]\tloss_supervised_1: 0.008406, loss_supervised_2: 0.004845\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tloss_supervised_1: 0.004359, loss_supervised_2: 0.004651\n",
            "Train Epoch: 5 [17280/60000 (29%)]\tloss_supervised_1: 0.071826, loss_supervised_2: 0.119408\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tloss_supervised_1: 0.000371, loss_supervised_2: 0.000438\n",
            "Train Epoch: 5 [18560/60000 (31%)]\tloss_supervised_1: 0.041558, loss_supervised_2: 0.059145\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tloss_supervised_1: 0.002712, loss_supervised_2: 0.001724\n",
            "Train Epoch: 5 [19840/60000 (33%)]\tloss_supervised_1: 0.000408, loss_supervised_2: 0.000364\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tloss_supervised_1: 0.003169, loss_supervised_2: 0.002622\n",
            "Train Epoch: 5 [21120/60000 (35%)]\tloss_supervised_1: 0.001144, loss_supervised_2: 0.000978\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tloss_supervised_1: 0.005667, loss_supervised_2: 0.001647\n",
            "Train Epoch: 5 [22400/60000 (37%)]\tloss_supervised_1: 0.095570, loss_supervised_2: 0.120710\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tloss_supervised_1: 0.034438, loss_supervised_2: 0.054642\n",
            "Train Epoch: 5 [23680/60000 (39%)]\tloss_supervised_1: 0.007778, loss_supervised_2: 0.005630\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tloss_supervised_1: 0.007887, loss_supervised_2: 0.008957\n",
            "Train Epoch: 5 [24960/60000 (42%)]\tloss_supervised_1: 0.024064, loss_supervised_2: 0.027104\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tloss_supervised_1: 0.000661, loss_supervised_2: 0.000527\n",
            "Train Epoch: 5 [26240/60000 (44%)]\tloss_supervised_1: 0.257011, loss_supervised_2: 0.275183\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tloss_supervised_1: 0.016920, loss_supervised_2: 0.025668\n",
            "Train Epoch: 5 [27520/60000 (46%)]\tloss_supervised_1: 0.009393, loss_supervised_2: 0.006214\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tloss_supervised_1: 0.002017, loss_supervised_2: 0.001885\n",
            "Train Epoch: 5 [28800/60000 (48%)]\tloss_supervised_1: 0.015738, loss_supervised_2: 0.020442\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tloss_supervised_1: 0.008532, loss_supervised_2: 0.005301\n",
            "Train Epoch: 5 [30080/60000 (50%)]\tloss_supervised_1: 0.141608, loss_supervised_2: 0.205904\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tloss_supervised_1: 0.005028, loss_supervised_2: 0.003534\n",
            "Train Epoch: 5 [31360/60000 (52%)]\tloss_supervised_1: 0.011699, loss_supervised_2: 0.010168\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tloss_supervised_1: 0.001056, loss_supervised_2: 0.000716\n",
            "Train Epoch: 5 [32640/60000 (54%)]\tloss_supervised_1: 0.011639, loss_supervised_2: 0.012373\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tloss_supervised_1: 0.081385, loss_supervised_2: 0.106462\n",
            "Train Epoch: 5 [33920/60000 (57%)]\tloss_supervised_1: 0.038740, loss_supervised_2: 0.049585\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tloss_supervised_1: 0.095627, loss_supervised_2: 0.112217\n",
            "Train Epoch: 5 [35200/60000 (59%)]\tloss_supervised_1: 0.017215, loss_supervised_2: 0.011937\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tloss_supervised_1: 0.007421, loss_supervised_2: 0.004013\n",
            "Train Epoch: 5 [36480/60000 (61%)]\tloss_supervised_1: 0.004443, loss_supervised_2: 0.011879\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tloss_supervised_1: 0.029136, loss_supervised_2: 0.031454\n",
            "Train Epoch: 5 [37760/60000 (63%)]\tloss_supervised_1: 0.059999, loss_supervised_2: 0.070629\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tloss_supervised_1: 0.000498, loss_supervised_2: 0.000647\n",
            "Train Epoch: 5 [39040/60000 (65%)]\tloss_supervised_1: 0.014394, loss_supervised_2: 0.026832\n",
            "Train Epoch: 5 [39680/60000 (66%)]\tloss_supervised_1: 0.148325, loss_supervised_2: 0.179898\n",
            "Train Epoch: 5 [40320/60000 (67%)]\tloss_supervised_1: 0.002945, loss_supervised_2: 0.001495\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tloss_supervised_1: 0.003597, loss_supervised_2: 0.002383\n",
            "Train Epoch: 5 [41600/60000 (69%)]\tloss_supervised_1: 0.048408, loss_supervised_2: 0.050943\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tloss_supervised_1: 0.000233, loss_supervised_2: 0.000196\n",
            "Train Epoch: 5 [42880/60000 (71%)]\tloss_supervised_1: 0.024525, loss_supervised_2: 0.039374\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tloss_supervised_1: 0.015181, loss_supervised_2: 0.025855\n",
            "Train Epoch: 5 [44160/60000 (74%)]\tloss_supervised_1: 0.046919, loss_supervised_2: 0.043523\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tloss_supervised_1: 0.039665, loss_supervised_2: 0.058466\n",
            "Train Epoch: 5 [45440/60000 (76%)]\tloss_supervised_1: 0.002697, loss_supervised_2: 0.001749\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tloss_supervised_1: 0.044357, loss_supervised_2: 0.037609\n",
            "Train Epoch: 5 [46720/60000 (78%)]\tloss_supervised_1: 0.000535, loss_supervised_2: 0.000440\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tloss_supervised_1: 0.021689, loss_supervised_2: 0.021464\n",
            "Train Epoch: 5 [48000/60000 (80%)]\tloss_supervised_1: 0.008003, loss_supervised_2: 0.003819\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tloss_supervised_1: 0.026179, loss_supervised_2: 0.022492\n",
            "Train Epoch: 5 [49280/60000 (82%)]\tloss_supervised_1: 0.000515, loss_supervised_2: 0.000573\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tloss_supervised_1: 0.000764, loss_supervised_2: 0.000627\n",
            "Train Epoch: 5 [50560/60000 (84%)]\tloss_supervised_1: 0.001595, loss_supervised_2: 0.001256\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tloss_supervised_1: 0.006148, loss_supervised_2: 0.002342\n",
            "Train Epoch: 5 [51840/60000 (86%)]\tloss_supervised_1: 0.033675, loss_supervised_2: 0.042310\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tloss_supervised_1: 0.003059, loss_supervised_2: 0.001438\n",
            "Train Epoch: 5 [53120/60000 (88%)]\tloss_supervised_1: 0.006065, loss_supervised_2: 0.003532\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tloss_supervised_1: 0.016387, loss_supervised_2: 0.020732\n",
            "Train Epoch: 5 [54400/60000 (91%)]\tloss_supervised_1: 0.089988, loss_supervised_2: 0.107478\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tloss_supervised_1: 0.161807, loss_supervised_2: 0.202847\n",
            "Train Epoch: 5 [55680/60000 (93%)]\tloss_supervised_1: 0.088342, loss_supervised_2: 0.133665\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tloss_supervised_1: 0.000501, loss_supervised_2: 0.000677\n",
            "Train Epoch: 5 [56960/60000 (95%)]\tloss_supervised_1: 0.065353, loss_supervised_2: 0.074296\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tloss_supervised_1: 0.009727, loss_supervised_2: 0.008029\n",
            "Train Epoch: 5 [58240/60000 (97%)]\tloss_supervised_1: 0.012413, loss_supervised_2: 0.012482\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tloss_supervised_1: 0.004443, loss_supervised_2: 0.002633\n",
            "Train Epoch: 5 [59520/60000 (99%)]\tloss_supervised_1: 0.001632, loss_supervised_2: 0.001223\n",
            "Max entropy given\n",
            "\n",
            "Test: Avg. loss: 0.0365, Accuracy: 9876/10000 (99%)\n",
            "\n",
            "True y given\n",
            "\n",
            "Test: Avg. loss: 0.0071, Accuracy: 9977/10000 (100%)\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor(99.7700, device='cuda:0')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "train_dataset = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('data', train=False, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "f = Net().to(device)\n",
        "optimizer = optim.Adam(f.parameters(), lr=1e-3)\n",
        "\n",
        "test(f, test_loader)\n",
        "train(f, train_loader, optimizer, n_epochs)\n",
        "test(f, test_loader)\n",
        "test(f, test_loader, with_true_y=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TTT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ttt_one_instance(x, f_ttt, f, optimizer, n_steps, y, n_classes=10):\n",
        "  f_ttt.load_state_dict(f.state_dict())  # reset f_ttt to f\n",
        "  f_ttt.train()\n",
        "  for step in range(n_steps):    \n",
        "    y0 = F.one_hot(torch.randint(0, n_classes, (x.shape[0],), device=device), num_classes=n_classes).float()\n",
        "    y1, z1 = f_ttt(x, y0)\n",
        "    y2, z2 = f.f2(z1, y1)\n",
        "    \n",
        "    loss_unsupervised_y = js_div(y1, y2)\n",
        "    loss_unsupervised_z = (z1 - z2).pow(2).mean()\n",
        "    loss = loss_unsupervised_y #+ loss_unsupervised_z\n",
        "        \n",
        "    \n",
        "    if y[0].item() != y1[0].argmax().item() and (step == 0 or step == n_steps - 1):\n",
        "      print(f'step {step}: loss={loss.item()}')\n",
        "      print(y0[0].argmax().item(), y1[0].argmax().item(), y2[0].argmax().item(), y[0].item())\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    if f_ttt.f2.fc2.weight.grad.var() == 0:\n",
        "      print('zero grad')\n",
        "    optimizer.step()\n",
        "  return y1, y2\n",
        "\n",
        "\n",
        "def ttt(f, test_loader, n_steps, lr):\n",
        "  f_ttt = deepcopy(f)\n",
        "  f.eval()\n",
        "  optimizer = optim.Adam(f_ttt.parameters(), lr=lr)\n",
        "  test_loss_1, correct_1 = 0, 0\n",
        "  test_loss_2, correct_2 = 0, 0\n",
        "\n",
        "  for ind, (data, target) in enumerate(test_loader):\n",
        "    print(f'batch {ind}/{len(test_loader)}:')\n",
        "    x, y = data.to(device), target.to(device)\n",
        "    y_hat_1, y_hat_2 = ttt_one_instance(x, f_ttt, f, optimizer, n_steps, y)\n",
        "\n",
        "    test_loss_1 += F.nll_loss(y_hat_1.log(), y, size_average=False).item()\n",
        "    test_loss_2 += F.nll_loss(y_hat_2.log(), y, size_average=False).item()\n",
        "\n",
        "    pred_1 = y_hat_1.data.max(1, keepdim=True)[1]\n",
        "    pred_2 = y_hat_2.data.max(1, keepdim=True)[1]\n",
        "\n",
        "    correct_1 += pred_1.eq(y.data.view_as(pred_1)).sum()\n",
        "    correct_2 += pred_2.eq(y.data.view_as(pred_2)).sum()\n",
        "\n",
        "  test_loss_1 /= len(test_loader.dataset)\n",
        "  test_loss_2 /= len(test_loader.dataset)\n",
        "\n",
        "\n",
        "\n",
        "  print('\\nttt y_hat_1: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss_1, correct_1, len(test_loader.dataset),\n",
        "    100. * correct_1 / len(test_loader.dataset)))\n",
        "  print('\\nttt y_hat_2: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss_2, correct_2, len(test_loader.dataset),\n",
        "    100. * correct_2 / len(test_loader.dataset)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def ttt_one_instance(x, f_ttt, f_copy, optimizer, n_steps, n_classes=10):\n",
        "#   for step in range(n_steps):\n",
        "#     f_copy.load_state_dict(f_ttt.state_dict())\n",
        "#     y_hat_0 = torch.ones((len(x), 10)).to(device) / n_classes\n",
        "#     y_hat_1, x_mid = f_ttt(x, y_hat_0, return_mid=True)\n",
        "#     y_hat_2 = f_copy.f2(x_mid, y_hat_1)\n",
        "#     y_hat_2_adv = f_ttt.f2(x_mid.detach(), y_hat_1.detach())\n",
        "    \n",
        "#     # loss_const = js_divergence(y_hat_1, y_hat_2)\n",
        "#     # loss_adv = -js_divergence(y_hat_1.detach(), y_hat_2_adv)*100\n",
        "#     loss_adv = -(y_hat_1.detach() - y_hat_2).pow(2).mean()*100\n",
        "#     loss_const = (y_hat_1 - y_hat_2).pow(2).mean()\n",
        "#     # args = y_hat_1.argmax(dim=1)\n",
        "#     # loss_adv = -(y_hat_1[args].detach() - y_hat_2_adv[args]).pow(2).mean()\n",
        "#     loss_entropy = -(y_hat_1 * torch.log(y_hat_1 + 1e-10)).sum(-1).mean()*0.01\n",
        "#     loss = loss_const + loss_adv + loss_entropy\n",
        "\n",
        "#     print(y_hat_1[0].argmax().item(), y_hat_2[0].argmax().item(), y_hat_2_adv[0].argmax().item())\n",
        "    \n",
        "#     # loss = (y_hat_1 - y_hat_2).pow(2).mean()\n",
        "    \n",
        "#     print(f'step {step}: loss={loss.item()}')\n",
        "#     optimizer.zero_grad()\n",
        "#     loss.backward()\n",
        "#     if f_ttt.f2.fc2.weight.grad.var() == 0:\n",
        "#       print('zero grad')\n",
        "#     optimizer.step()\n",
        "#   return y_hat_1, y_hat_2\n",
        "\n",
        "\n",
        "# def ttt(f, test_loader, n_steps, lr):\n",
        "#   f_ttt = deepcopy(f)\n",
        "#   f_copy = deepcopy(f)\n",
        "#   f.eval()\n",
        "#   f_copy.train()\n",
        "#   f_ttt.train()\n",
        "#   optimizer = optim.Adam(f_ttt.parameters(), lr=lr)\n",
        "#   test_loss_1, correct_1, test_losses_1 = 0, 0, []\n",
        "#   test_loss_2, correct_2, test_losses_2 = 0, 0, []\n",
        "\n",
        "#   for data, target in test_loader:\n",
        "#     x, y = data.to(device), target.to(device)\n",
        "#     f_ttt.load_state_dict(f.state_dict())  # reset f_ttt to f\n",
        "#     f_ttt.train()\n",
        "\n",
        "#     y_hat_1, y_hat_2 = ttt_one_instance(x, f_ttt, f_copy, optimizer, n_steps)\n",
        "\n",
        "#     test_loss_1 += F.nll_loss(y_hat_1.log(), y, size_average=False).item()\n",
        "#     test_loss_2 += F.nll_loss(y_hat_2.log(), y, size_average=False).item()\n",
        "\n",
        "#     pred_1 = y_hat_1.data.max(1, keepdim=True)[1]\n",
        "#     pred_2 = y_hat_2.data.max(1, keepdim=True)[1]\n",
        "\n",
        "#     correct_1 += pred_1.eq(y.data.view_as(pred_1)).sum()\n",
        "#     correct_2 += pred_2.eq(y.data.view_as(pred_2)).sum()\n",
        "\n",
        "#   test_loss_1 /= len(test_loader.dataset)\n",
        "#   test_loss_2 /= len(test_loader.dataset)\n",
        "\n",
        "#   test_losses_1.append(test_loss_1)\n",
        "#   test_losses_2.append(test_loss_2)\n",
        "\n",
        "#   print('\\nttt y_hat_1: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "#     test_loss_1, correct_1, len(test_loader.dataset),\n",
        "#     100. * correct_1 / len(test_loader.dataset)))\n",
        "#   print('\\nttt y_hat_2: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "#     test_loss_2, correct_2, len(test_loader.dataset),\n",
        "#     100. * correct_2 / len(test_loader.dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max entropy given\n",
            "\n",
            "Test: Avg. loss: 0.7275, Accuracy: 7525/10000 (75%)\n",
            "\n",
            "batch 0/157:\n",
            "batch 1/157:\n",
            "batch 2/157:\n",
            "batch 3/157:\n",
            "batch 4/157:\n",
            "batch 5/157:\n",
            "step 0: loss=0.012889975681900978\n",
            "8 8 8 9\n",
            "step 11: loss=0.004932774696499109\n",
            "7 7 7 9\n",
            "batch 6/157:\n",
            "batch 7/157:\n",
            "step 0: loss=0.012837264686822891\n",
            "9 8 8 9\n",
            "step 11: loss=0.004485766869038343\n",
            "7 3 3 9\n",
            "batch 8/157:\n",
            "batch 9/157:\n",
            "batch 10/157:\n",
            "batch 11/157:\n",
            "batch 12/157:\n",
            "batch 13/157:\n",
            "batch 14/157:\n",
            "batch 15/157:\n",
            "step 0: loss=0.01212082989513874\n",
            "5 5 5 7\n",
            "batch 16/157:\n",
            "step 0: loss=0.012450642883777618\n",
            "6 8 8 4\n",
            "batch 17/157:\n",
            "batch 18/157:\n",
            "step 0: loss=0.01734895259141922\n",
            "2 8 8 9\n",
            "batch 19/157:\n",
            "step 0: loss=0.012396005913615227\n",
            "1 1 1 7\n",
            "step 11: loss=0.0034492812119424343\n",
            "0 1 1 7\n",
            "batch 20/157:\n",
            "step 0: loss=0.010795621201395988\n",
            "7 7 7 1\n",
            "step 11: loss=0.0032694111578166485\n",
            "3 2 3 1\n",
            "batch 21/157:\n",
            "batch 22/157:\n",
            "batch 23/157:\n",
            "step 11: loss=0.004042170941829681\n",
            "9 5 5 3\n",
            "batch 24/157:\n",
            "step 0: loss=0.010347122326493263\n",
            "7 8 8 6\n",
            "batch 25/157:\n",
            "step 0: loss=0.010273238644003868\n",
            "1 5 5 3\n",
            "batch 26/157:\n",
            "batch 27/157:\n",
            "batch 28/157:\n",
            "batch 29/157:\n",
            "batch 30/157:\n",
            "step 0: loss=0.014235662296414375\n",
            "3 3 3 4\n",
            "step 11: loss=0.004574380815029144\n",
            "7 7 7 4\n",
            "batch 31/157:\n",
            "batch 32/157:\n",
            "batch 33/157:\n",
            "batch 34/157:\n",
            "batch 35/157:\n",
            "batch 36/157:\n",
            "batch 37/157:\n",
            "batch 38/157:\n",
            "step 11: loss=0.002844536677002907\n",
            "6 3 2 2\n",
            "batch 39/157:\n",
            "step 0: loss=0.012528527528047562\n",
            "9 3 3 2\n",
            "step 11: loss=0.004408726468682289\n",
            "1 3 3 2\n",
            "batch 40/157:\n",
            "batch 41/157:\n",
            "batch 42/157:\n",
            "batch 43/157:\n",
            "batch 44/157:\n",
            "step 0: loss=0.011786076240241528\n",
            "6 6 2 1\n",
            "batch 45/157:\n",
            "batch 46/157:\n",
            "step 0: loss=0.013047395274043083\n",
            "0 2 2 0\n",
            "step 11: loss=0.0039023789577186108\n",
            "9 2 2 0\n",
            "batch 47/157:\n",
            "batch 48/157:\n",
            "batch 49/157:\n",
            "step 0: loss=0.009515700861811638\n",
            "6 8 8 7\n",
            "batch 50/157:\n",
            "batch 51/157:\n",
            "step 0: loss=0.011973552405834198\n",
            "2 2 2 1\n",
            "batch 52/157:\n",
            "batch 53/157:\n",
            "batch 54/157:\n",
            "batch 55/157:\n",
            "step 0: loss=0.012475136667490005\n",
            "1 4 4 6\n",
            "step 11: loss=0.005013930611312389\n",
            "7 4 4 6\n",
            "batch 56/157:\n",
            "batch 57/157:\n",
            "step 0: loss=0.014188382774591446\n",
            "7 7 7 1\n",
            "step 11: loss=0.004183796234428883\n",
            "3 3 3 1\n",
            "batch 58/157:\n",
            "batch 59/157:\n",
            "step 0: loss=0.012845010496675968\n",
            "3 8 8 5\n",
            "step 11: loss=0.004080225247889757\n",
            "7 8 8 5\n",
            "batch 60/157:\n",
            "batch 61/157:\n",
            "batch 62/157:\n",
            "step 11: loss=0.005120991729199886\n",
            "8 8 9 5\n",
            "batch 63/157:\n",
            "step 11: loss=0.005368003621697426\n",
            "8 8 8 1\n",
            "batch 64/157:\n",
            "step 0: loss=0.014710240066051483\n",
            "0 3 3 8\n",
            "step 11: loss=0.0050180284306406975\n",
            "6 3 3 8\n",
            "batch 65/157:\n",
            "batch 66/157:\n",
            "step 0: loss=0.007072308100759983\n",
            "9 7 7 9\n",
            "step 11: loss=0.0042061638087034225\n",
            "6 7 7 9\n",
            "batch 67/157:\n",
            "batch 68/157:\n",
            "batch 69/157:\n",
            "step 0: loss=0.014104575850069523\n",
            "7 2 5 6\n",
            "batch 70/157:\n",
            "step 0: loss=0.011461688205599785\n",
            "5 5 5 9\n",
            "batch 71/157:\n",
            "batch 72/157:\n",
            "batch 73/157:\n",
            "step 0: loss=0.013987168669700623\n",
            "2 2 2 0\n",
            "step 11: loss=0.003874398535117507\n",
            "1 2 2 0\n",
            "batch 74/157:\n",
            "batch 75/157:\n",
            "batch 76/157:\n",
            "step 0: loss=0.012297838926315308\n",
            "3 3 3 1\n",
            "step 11: loss=0.004885201342403889\n",
            "6 2 2 1\n",
            "batch 77/157:\n",
            "batch 78/157:\n",
            "batch 79/157:\n",
            "batch 80/157:\n",
            "batch 81/157:\n",
            "batch 82/157:\n",
            "batch 83/157:\n",
            "batch 84/157:\n",
            "batch 85/157:\n",
            "batch 86/157:\n",
            "batch 87/157:\n",
            "batch 88/157:\n",
            "batch 89/157:\n",
            "step 0: loss=0.009611311368644238\n",
            "2 2 2 4\n",
            "step 11: loss=0.004281170200556517\n",
            "0 2 3 4\n",
            "batch 90/157:\n",
            "batch 91/157:\n",
            "batch 92/157:\n",
            "batch 93/157:\n",
            "batch 94/157:\n",
            "batch 95/157:\n",
            "batch 96/157:\n",
            "batch 97/157:\n",
            "step 0: loss=0.009557011537253857\n",
            "2 3 3 7\n",
            "batch 98/157:\n",
            "batch 99/157:\n",
            "batch 100/157:\n",
            "step 0: loss=0.011514759622514248\n",
            "2 4 4 0\n",
            "step 11: loss=0.003830916481092572\n",
            "1 6 6 0\n",
            "batch 101/157:\n",
            "batch 102/157:\n",
            "step 0: loss=0.01263456605374813\n",
            "3 3 3 2\n",
            "batch 103/157:\n",
            "step 11: loss=0.005681563634425402\n",
            "2 2 2 9\n",
            "batch 104/157:\n",
            "batch 105/157:\n",
            "step 0: loss=0.014095139689743519\n",
            "3 3 3 1\n",
            "step 11: loss=0.0028268219903111458\n",
            "5 8 8 1\n",
            "batch 106/157:\n",
            "step 0: loss=0.010332135483622551\n",
            "5 8 8 9\n",
            "step 11: loss=0.0020043859258294106\n",
            "9 8 8 9\n",
            "batch 107/157:\n",
            "batch 108/157:\n",
            "step 0: loss=0.012213990092277527\n",
            "1 3 3 2\n",
            "step 11: loss=0.0022994214668869972\n",
            "4 4 4 2\n",
            "batch 109/157:\n",
            "step 11: loss=0.002940540900453925\n",
            "2 2 1 1\n",
            "batch 110/157:\n",
            "batch 111/157:\n",
            "step 11: loss=0.0018969631055369973\n",
            "4 5 5 3\n",
            "batch 112/157:\n",
            "batch 113/157:\n",
            "batch 114/157:\n",
            "batch 115/157:\n",
            "batch 116/157:\n",
            "step 0: loss=0.010597513988614082\n",
            "1 8 8 1\n",
            "step 11: loss=0.004628273658454418\n",
            "8 8 8 1\n",
            "batch 117/157:\n",
            "batch 118/157:\n",
            "batch 119/157:\n",
            "step 0: loss=0.012487029656767845\n",
            "8 8 8 9\n",
            "batch 120/157:\n",
            "step 11: loss=0.003917197696864605\n",
            "2 2 2 9\n",
            "batch 121/157:\n",
            "step 0: loss=0.010923333466053009\n",
            "4 3 3 7\n",
            "batch 122/157:\n",
            "batch 123/157:\n",
            "batch 124/157:\n",
            "batch 125/157:\n",
            "batch 126/157:\n",
            "step 0: loss=0.012746002525091171\n",
            "0 8 8 7\n",
            "batch 127/157:\n",
            "step 0: loss=0.011123468168079853\n",
            "2 2 2 1\n",
            "step 11: loss=0.0019378478173166513\n",
            "0 8 8 1\n",
            "batch 128/157:\n",
            "batch 129/157:\n",
            "step 0: loss=0.016682792454957962\n",
            "6 2 6 6\n",
            "batch 130/157:\n",
            "step 0: loss=0.012344928458333015\n",
            "8 3 3 2\n",
            "step 11: loss=0.00249651400372386\n",
            "8 3 3 2\n",
            "batch 131/157:\n",
            "batch 132/157:\n",
            "batch 133/157:\n",
            "batch 134/157:\n",
            "batch 135/157:\n",
            "batch 136/157:\n",
            "batch 137/157:\n",
            "batch 138/157:\n",
            "batch 139/157:\n",
            "batch 140/157:\n",
            "batch 141/157:\n",
            "step 0: loss=0.01026062946766615\n",
            "4 3 3 7\n",
            "batch 142/157:\n",
            "batch 143/157:\n",
            "batch 144/157:\n",
            "step 0: loss=0.008709208108484745\n",
            "3 8 8 9\n",
            "batch 145/157:\n",
            "batch 146/157:\n",
            "batch 147/157:\n",
            "batch 148/157:\n",
            "batch 149/157:\n",
            "batch 150/157:\n",
            "batch 151/157:\n",
            "step 0: loss=0.015535383485257626\n",
            "3 3 3 2\n",
            "step 11: loss=0.006472491659224033\n",
            "4 3 3 2\n",
            "batch 152/157:\n",
            "step 0: loss=0.01521517988294363\n",
            "7 7 7 4\n",
            "step 11: loss=0.0057311696000397205\n",
            "1 8 8 4\n",
            "batch 153/157:\n",
            "step 0: loss=0.01127433031797409\n",
            "5 5 5 4\n",
            "step 11: loss=0.005300382152199745\n",
            "0 0 0 4\n",
            "batch 154/157:\n",
            "step 11: loss=0.005113719031214714\n",
            "4 4 4 9\n",
            "batch 155/157:\n",
            "batch 156/157:\n",
            "step 0: loss=0.00853861216455698\n",
            "6 2 2 1\n",
            "step 11: loss=0.006538676097989082\n",
            "7 7 7 1\n",
            "\n",
            "ttt y_hat_1: Avg. loss: 0.8090, Accuracy: 7907/10000 (79%)\n",
            "\n",
            "\n",
            "ttt y_hat_2: Avg. loss: 0.8382, Accuracy: 7894/10000 (79%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "class AddGaussianNoise(object):\n",
        "    def __init__(self, std, mean=0.):\n",
        "        self.std = std\n",
        "        self.mean = mean\n",
        "        \n",
        "    def __call__(self, tensor):\n",
        "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,)),\n",
        "    AddGaussianNoise(1.75)])\n",
        "ood_dataset = datasets.MNIST('data', train=False, download=True, transform=transform)\n",
        "ood_loader = torch.utils.data.DataLoader(ood_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "test(f, ood_loader)\n",
        "ttt(f, ood_loader, n_steps=12, lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'network' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test(\u001b[43mnetwork\u001b[49m, test_loader)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'network' is not defined"
          ]
        }
      ],
      "source": [
        "test(network, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
