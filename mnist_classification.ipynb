{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5Jzaqkb-acN",
        "outputId": "404c3b90-6f88-46d9-faef-942446c8da9c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "from copy import deepcopy\n",
        "\n",
        "n_epochs = 5 # number of epochs for training\n",
        "batch_size_train = 1024 # batch size for training\n",
        "batch_size_test = 8192 # batch size for testing\n",
        "learning_rate = 0.001 # learning rate for Adam\n",
        "log_interval = 10 # logging interval for metrics\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # device for computation\n",
        "\n",
        "# fixing random seed\n",
        "random_seed = 1\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.manual_seed(random_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsTQf0xB-jGr"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.f1 = F1()\n",
        "        self.f2 = F2()\n",
        "    \n",
        "    def forward(self, x, y_hat, return_mid=False):\n",
        "        z = self.f1(x)\n",
        "        y_pred, z_pred  = self.f2(z, y_hat)\n",
        "        return y_pred, z_pred\n",
        "    \n",
        "    \n",
        "class F1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(F1, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.activation = nn.LeakyReLU(negative_slope=0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = self.activation(F.max_pool2d(self.conv2(x), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = self.activation(self.fc1(x))\n",
        "        return x\n",
        "    \n",
        "    \n",
        "class F2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(F2, self).__init__()\n",
        "        self.fc2 = nn.Linear(50, 50)\n",
        "        self.fc3 = nn.Linear(50, 60)\n",
        "        self.y_hat_fc = nn.Linear(10, 50)\n",
        "        self.activation = nn.LeakyReLU(negative_slope=0.01)\n",
        "\n",
        "    def forward(self, x, y_hat):\n",
        "        x = x + self.activation(self.y_hat_fc(y_hat))\n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x[:, :10].softmax(-1), x[:, 10:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def js_div(p, q):\n",
        "    m = 0.5 * (p + q)\n",
        "    return 0.5 * (F.kl_div(torch.log(p), m, reduction='batchmean') + \n",
        "                  F.kl_div(torch.log(q), m, reduction='batchmean'))\n",
        "\n",
        "def train(f, train_loader, optimizer, n_epochs, n_classes=10):\n",
        "  f.train()\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "      # randomally choose y0 to be either the true target y or a random class\n",
        "      x, y = data.to(device), target.to(device)\n",
        "      y0_d = y if torch.rand(1) > 0.25 else torch.randint(0, n_classes, (x.shape[0],), device=device)\n",
        "      y0 = F.one_hot(y0_d, num_classes=n_classes).float()\n",
        "\n",
        "      # forward pass\n",
        "      y1, z1 = f(x, y0)\n",
        "      y2, z2 = f.f2(z1, y1)\n",
        "\n",
        "      # losses\n",
        "      loss_supervised_1 = F.nll_loss(y1.log(), y)\n",
        "      loss_supervised_2 = F.nll_loss(y2.log(), y)\n",
        "      loss_unsupervised_y = js_div(y1, y2)*10\n",
        "      loss_unsupervised_z = (z1 - z2).pow(2).mean()*10\n",
        "      # loss = loss_supervised + #loss_unsupervised_y # + loss_unsupervised_z\n",
        "      loss = loss_supervised_1 + loss_supervised_2\n",
        "\n",
        "      # opt\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # log\n",
        "      if batch_idx % log_interval == 0:\n",
        "        # print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, loss_supervised: {:.6f}, loss_unsupervised_y: {:.6f}, loss_unsupervised_z: {:.6f}'.format(\n",
        "        #   epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "        #   100. * batch_idx / len(train_loader), loss.item(), loss_supervised.item(), loss_unsupervised_y.item(), loss_unsupervised_z.item()))\n",
        "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tloss_supervised_1: {:.6f}, loss_supervised_2: {:.6f}'.format(\n",
        "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "          100. * batch_idx / len(train_loader), loss_supervised_1.item(), loss_supervised_2.item()))\n",
        "        torch.save(f.state_dict(), './model.pth')\n",
        "        torch.save(optimizer.state_dict(), './optimizer.pth')\n",
        "      \n",
        "\n",
        "def test(f, test_loader, n_classes=10, with_true_y=False):\n",
        "  f.eval()\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      x, y = data.to(device), target.to(device)\n",
        "      if with_true_y:\n",
        "        y0 = F.one_hot(y, num_classes=n_classes).float()\n",
        "      else:\n",
        "        y0 = torch.ones((len(y), n_classes), device=x.device).to(device).softmax(-1)\n",
        "      y1, z1 = f(x, y0)\n",
        "      test_loss += F.nll_loss(y1.log(), y, size_average=False).item()\n",
        "      pred = y1.log().data.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(y.data.view_as(pred)).sum()\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  print(\"True y given\") if with_true_y else print(\"Max entropy given\")\n",
        "  print('\\nTest: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss, correct, len(test_loader.dataset),\n",
        "    100. * correct / len(test_loader.dataset)))\n",
        "  return 100 * correct / len(test_loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_IS4C6B_HMp",
        "outputId": "b5252423-4826-4d3a-bc1d-c84a04b8dd6c"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "train_dataset = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('data', train=False, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "f = Net().to(device)\n",
        "optimizer = optim.Adam(f.parameters(), lr=1e-3)\n",
        "\n",
        "test(f, test_loader)\n",
        "train(f, train_loader, optimizer, n_epochs)\n",
        "test(f, test_loader)\n",
        "test(f, test_loader, with_true_y=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TTT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ttt_one_instance(x, f_ttt, f, optimizer, n_steps, y, n_classes=10):\n",
        "  f_ttt.load_state_dict(f.state_dict())  # reset f_ttt to f\n",
        "  f_ttt.train()\n",
        "  for step in range(n_steps):    \n",
        "    y0 = F.one_hot(torch.randint(0, n_classes, (x.shape[0],), device=device), num_classes=n_classes).float()\n",
        "    y1, z1 = f_ttt(x, y0)\n",
        "    y2, z2 = f.f2(z1, y1)\n",
        "    \n",
        "    loss_unsupervised_y = js_div(y1, y2)\n",
        "    loss_unsupervised_z = (z1 - z2).pow(2).mean()\n",
        "    loss = loss_unsupervised_y #+ loss_unsupervised_z\n",
        "        \n",
        "    \n",
        "    if y[0].item() != y1[0].argmax().item() and (step == 0 or step == n_steps - 1):\n",
        "      print(f'step {step}: loss={loss.item()}')\n",
        "      print(y0[0].argmax().item(), y1[0].argmax().item(), y2[0].argmax().item(), y[0].item())\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    if f_ttt.f2.fc2.weight.grad.var() == 0:\n",
        "      print('zero grad')\n",
        "    optimizer.step()\n",
        "  return y1, y2\n",
        "\n",
        "\n",
        "def ttt(f, test_loader, n_steps, lr):\n",
        "  f_ttt = deepcopy(f)\n",
        "  f.eval()\n",
        "  optimizer = optim.Adam(f_ttt.parameters(), lr=lr)\n",
        "  test_loss_1, correct_1 = 0, 0\n",
        "  test_loss_2, correct_2 = 0, 0\n",
        "\n",
        "  for ind, (data, target) in enumerate(test_loader):\n",
        "    print(f'batch {ind}/{len(test_loader)}:')\n",
        "    x, y = data.to(device), target.to(device)\n",
        "    y_hat_1, y_hat_2 = ttt_one_instance(x, f_ttt, f, optimizer, n_steps, y)\n",
        "\n",
        "    test_loss_1 += F.nll_loss(y_hat_1.log(), y, size_average=False).item()\n",
        "    test_loss_2 += F.nll_loss(y_hat_2.log(), y, size_average=False).item()\n",
        "\n",
        "    pred_1 = y_hat_1.data.max(1, keepdim=True)[1]\n",
        "    pred_2 = y_hat_2.data.max(1, keepdim=True)[1]\n",
        "\n",
        "    correct_1 += pred_1.eq(y.data.view_as(pred_1)).sum()\n",
        "    correct_2 += pred_2.eq(y.data.view_as(pred_2)).sum()\n",
        "\n",
        "  test_loss_1 /= len(test_loader.dataset)\n",
        "  test_loss_2 /= len(test_loader.dataset)\n",
        "\n",
        "\n",
        "\n",
        "  print('\\nttt y_hat_1: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss_1, correct_1, len(test_loader.dataset),\n",
        "    100. * correct_1 / len(test_loader.dataset)))\n",
        "  print('\\nttt y_hat_2: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss_2, correct_2, len(test_loader.dataset),\n",
        "    100. * correct_2 / len(test_loader.dataset)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AddGaussianNoise(object):\n",
        "    def __init__(self, std, mean=0.):\n",
        "        self.std = std\n",
        "        self.mean = mean\n",
        "        \n",
        "    def __call__(self, tensor):\n",
        "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,)),\n",
        "    AddGaussianNoise(1.75)])\n",
        "ood_dataset = datasets.MNIST('data', train=False, download=True, transform=transform)\n",
        "ood_loader = torch.utils.data.DataLoader(ood_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "test(f, ood_loader)\n",
        "ttt(f, ood_loader, n_steps=12, lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
